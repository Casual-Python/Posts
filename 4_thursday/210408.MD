**Нестандартные массивы**

Сегодня я хочу обратить внимание не на обычные [тензоры](https://www.tensorflow.org/guide/tensor#ragged_tensors), а на их модификации [Ragged Tensors](https://www.tensorflow.org/guide/ragged_tensor) и [Sparse Tensors](https://www.tensorflow.org/guide/sparse_tensor). Поймем, что это такое, хорошо ли работают и подумаем о том, где же их применить.

Код для тестов можно посмотреть [здесь](https://github.com/Casual-Python/Posts/blob/main/4_thursday/080421.py)!

1) Ragged Tensors - переводится как рваный или неровный, что в принципе и отражает его суть. Этот тензор может иметь неравномерный shape. Создадим его из листа и посмотрим на shape:

```
tf.ragged.constant([[1,2,3],[2]]).shape
TensorShape([2, None])
```

В shape начиная со второго элемента стоит None, что и показывает неравномерность. В то же время есть ограничение на то, чтобы глубина тензора была одинакова:

```
tf.ragged.constant([2,[1]])
ValueError: all scalar values must have the same nesting depth
```

Эти ограничения обусловлены тем, что в реализации используются особое представление [индексов](https://www.tensorflow.org/guide/ragged_tensor#raggedtensor_encoding).
Посмотрим на то, что позволяет делать такой тип тензоров. В переменную tf_text загружен текст в формате параграф-предложение-слово. Вытащить первые 3 слова из последнего предложения каждого 35 параграфа можно вот таким легким движением:

```
tf_text[::35,-1:,:3]
<tf.RaggedTensor [[[b'Lorem', b'ipsum', b'dolor']], [[b'Nunc', b'lobortis', b'cursus']], [[b'Ut', b'nibh', b'lectus']]]>
```

А если вам захочется сделать все слова в нижнем регистре, то будет достаточно:
```
tf.ragged.map_flat_values(tf.strings.lower, tf_text)
<tf.RaggedTensor [[[b'lorem'...]]]>
```
Таким образом мы получаем достаточно удобный способ работать с данными разной длины. Можно использовать, например, для анализа текста.

2) Sparse Tensors - тут уже есть официальный перевод - [разреженный](https://ru.wikipedia.org/wiki/Разряженнаяматрица). Это тип тензора подходит, когда в данных много нулей. Тогда можно воспользоваться этим и хранить только оставшиеся элементы. Для генерации можно использовать стандартный лист или numpy.array, а также перечисления элементов. Подходит для encoding, когда остается мало элементов.
Протестируем, чтобы оценить возможный прирост в производительности при вычислении этих тензоров. Для анализа будем генерировать вот так как биномиальное выдающее 0 или 1 и пуассона для случайно натурального числа:

```
test_tensor = tf.constant(np.random.binomial(...)*np.random.poisson(...))
sparse_test_tensor = tf.sparse.from_dense(test_tensor)
```

Тут p_val определяет число нулей в полученном тензоре, число элементов в строке 3E3, суммарно 9E6. Для теста перемножим матрично этот тензор сам с собой. Будем варьировать p_val и смотреть на время вычисления. Конкретные данные приведены на графике. В результате теста видно, что время вычисления стандартного фиксировано, в то время как sparse дает заметный выигрыш только для маленького p_val. В данном случае получилось, что выгодно использовать только при заполненности менее 6.5%, а потом проигрыш становится огромным. Это сходится с тем, что алгоритмически обычное умножение зависит только от размера тензора, а sparse от числа элементов линейно.